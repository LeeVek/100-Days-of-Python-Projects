{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPP8WpKtOBbUbXTwmAR9bI5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeeVek/100-Days-of-Python-Projects/blob/main/D4_Imbalanced_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   choose appropriate performance metric --accuracy,recall,precision,area under curve\n",
        "2.   stratify imbalanced dataset into train-test split\n",
        "3.   use data sampling algorithms like SMOTE  k-folding cross validation\n",
        "4.   Algo from field of cost sensitive learning\n",
        "5.   Use modified svm and decision tress\n",
        "6.   Tune threshold when interpreting predicted probabilities\n",
        "\n"
      ],
      "metadata": {
        "id": "izax3Jq6SlsC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**K-fold cross-validation** is a technique used in machine learning and **model evaluation** to assess the **performance** and generalization ability of a **predictive model**.\n",
        "\n",
        "In k-fold cross-validation, the original dataset is divided into **k subsets** or folds of approximately **equal size**. \n",
        "\n",
        "The model is **trained on k-1 folds of the data,** and the **remaining fold is used for validation.**\n",
        "The model's performance is evaluated using a chosen **evaluation metric on the validation fold.**\n",
        "Steps 2 and 3 are repeated k times, with each fold serving as the validation set once.\n",
        "The **performance** measures obtained from each iteration are **averaged** to provide an overall evaluation of the model's performance.\n",
        "\n",
        "by **reducing the bias** introduced by a **single train-test split**. It allows for a more robust assessment of the model's ability to generalize to unseen data.\n",
        "\n",
        "Common choices for the value of k are 5 and 10.\n",
        "\n",
        " identify potential issues like **overfitting or underfittin**g, and make more informed decisions about model **selection and hyperparameter tuning**."
      ],
      "metadata": {
        "id": "CYOnAoADTZta"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " two common sampling techniques used with imbalanced data:\n",
        "\n",
        "**Undersampling**: s reducing the number of instances from the majority class to match the number of instances in the minority class. Tomek links or Cluster Centroids.\n",
        "\n",
        "**Oversampling**: increasing the number of instances in the minority class to match the number of instances in the majority class. This can be done by **replicating** existing instances from the minority class or generating **synthetic samples** using techniques like** SMOTE (Synthetic Minority Over-sampling Technique)** or **ADASYN (Adaptive Synthetic Sampling)**. \n",
        "\n",
        "Oversampling can potentially lead to overfitting, while undersampling may discard valuable information from the majority class. Additionally, alternative approaches like **cost-sensitive learning, ensemble methods, or using evaluation metrics** that focus on the minority class can also be considered when working with imbalanced data."
      ],
      "metadata": {
        "id": "P5G5WZwuXs-0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cost-sensitive learning** takes into account the varying costs of **misclassification** and incorporates them into the learning process. The goal is to optimize the model's performance by minimizing the overall cost of **misclassification** rather than just the error rate or accuracy.\n",
        "\n",
        "1.  Misclassification costs\n",
        "2.  Reweighting instances\n",
        "3.  Algorithm modification\n",
        "\n"
      ],
      "metadata": {
        "id": "_77dDO2LYde0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FwnvFJ1Sd5r"
      },
      "outputs": [],
      "source": []
    }
  ]
}